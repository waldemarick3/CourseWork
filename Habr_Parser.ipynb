{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42eddbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from datetime import timedelta\n",
    "import dateparser\n",
    "from requests import Session\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from requests_futures.sessions import FuturesSession\n",
    "\n",
    "\n",
    "def fetch_raw_habr_pages_requests_futures(pages=10):\n",
    "    ''' Получить сырые данные с хабра '''\n",
    "    session = FuturesSession(\n",
    "        executor=ThreadPoolExecutor(max_workers=20),\n",
    "        session=Session()\n",
    "    )\n",
    "    pages_habr = []\n",
    "    for page_number in range(1, pages+1):\n",
    "        r = session.get(f\"https://habr.com/ru/all/page{page_number}/\",headers={\n",
    "\"User-Agent\" : \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36\"\n",
    "})\n",
    "        pages_habr.append(r)\n",
    "    pages_habr = [r.result().text for r in pages_habr]\n",
    "    return pages_habr\n",
    "\n",
    "\n",
    "def fetch_raw_habr_pages(pages=10):\n",
    "    ''' Получить сырые данные с хабра '''\n",
    "    return fetch_raw_habr_pages_requests_futures(pages)\n",
    "\n",
    "\n",
    "def convert_habr_date_to_datetime(date_habr):\n",
    "    ''' Конвертер строковой даты с habr.com  в datetime '''\n",
    "    return dateparser.parse(date_habr)\n",
    "\n",
    "\n",
    "def get_titles_articles_with_raw_habr_pages(habr_pages):\n",
    "    ''' Распарсить сырые страницы: выбрать заголовки статей с датами\n",
    "        публикации '''\n",
    "    titles_articles = []\n",
    "    for page_text in habr_pages:\n",
    "        soup = BeautifulSoup(page_text, \"html.parser\")\n",
    "        \n",
    "        articles = soup.find_all(\"div\", class_=\"tm-article-snippet tm-article-snippet\")\n",
    "        for article in articles:\n",
    "            date_of_publication_tag = article.find(\"time\")\n",
    "    \n",
    "            if not date_of_publication_tag:\n",
    "                continue\n",
    "            # Конвертируем её в datetime\n",
    "            date_of_publication = convert_habr_date_to_datetime(\n",
    "                date_of_publication_tag.get_text()\n",
    "            )\n",
    "            \n",
    "            title_article_tag = article.find(\"a\", class_=\"tm-article-snippet__title-link\")\n",
    "            title_article_tag = title_article_tag.find(\"span\")\n",
    "            if not title_article_tag:\n",
    "                continue\n",
    "\n",
    "            title_article = title_article_tag.get_text()\n",
    "\n",
    "            titles_articles.append(\n",
    "                {'date': date_of_publication,\n",
    "                 'title': title_article}\n",
    "            )\n",
    "    return titles_articles\n",
    "\n",
    "\n",
    "def get_weeks(date_begin, date_end):\n",
    "    # Формируем список недель\n",
    "    data_cur = date_begin - timedelta(days=date_begin.weekday())\n",
    "    delta = date_end - data_cur\n",
    "    return [\n",
    "        (data_cur + timedelta(days=i), data_cur + timedelta(days=i+1))\n",
    "        for i in range(0, delta.days, 1)\n",
    "    ]\n",
    "\n",
    "\n",
    "def divide_titles_at_weeks(titles_articles,):\n",
    "    ''' Разделить заголовки по неделям '''\n",
    "    if not titles_articles:\n",
    "        return []\n",
    "    # Берём даты публикаций самой старой и самой свежей статьи\n",
    "    date_begin = titles_articles[-1]['date'].date()\n",
    "    date_end = titles_articles[0]['date'].date()\n",
    "\n",
    "    # Формируем список недель\n",
    "    weeks = get_weeks(date_begin, date_end)\n",
    "\n",
    "    # Разбиваем статьи по неделям\n",
    "    titles_articles_weeks = []\n",
    "    for date_begin_week, date_end_week in weeks:\n",
    "        titles_articles_weeks.append({\n",
    "            'date': date_begin_week,\n",
    "            'titles_articles': [\n",
    "                title_article for title_article in titles_articles\n",
    "                if (title_article['date'].date() >= date_begin_week and\n",
    "                    title_article['date'].date() < date_end_week)\n",
    "            ]\n",
    "        })\n",
    "    return titles_articles_weeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5da55e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b465d9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "import re\n",
    "import collections\n",
    "\n",
    "\n",
    "def flat(not_flat_list):\n",
    "    \"\"\" [(1,2), (3,4)] -> [1, 2, 3, 4]\"\"\"\n",
    "    return [item for sublist in not_flat_list for item in sublist]\n",
    "\n",
    "\n",
    "def parse_nouns_in_titles_articles(titles_articles, morph=None):\n",
    "    ''' Выбрать существительные.'''\n",
    "\n",
    "    # Разбиваем заголовок на слова\n",
    "    #Очищаем строку от \"мусора\"\n",
    "    words = flat([\n",
    "        re.sub('[^a-zA-Zа-яА-Я0-9 -]', '', title_article['title']).split()\n",
    "        for title_article in titles_articles['titles_articles']\n",
    "    ])\n",
    "\n",
    "    if not morph:\n",
    "        morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "    nouns = []\n",
    "    for word in words:\n",
    "        # Морфологический разбор\n",
    "        word_parses = morph.parse(word)\n",
    "        if word_parses:\n",
    "            if ('NOUN' in word_parses[0].tag or\n",
    "                    'LATN' in word_parses[0].tag):\n",
    "                nouns.append(word_parses[0].normal_form)\n",
    "    return nouns\n",
    "\n",
    "\n",
    "def parse_nouns_in_titles_articles_at_weeks(titles_articles_weeks, top_size):\n",
    "    '''Выбрать самые популярные существительные по неделям.'''\n",
    "    top_nouns_weeks = []\n",
    "    morph = pymorphy2.MorphAnalyzer()\n",
    "    for titles_articles in titles_articles_weeks:\n",
    "        nouns = parse_nouns_in_titles_articles(titles_articles, morph)\n",
    "        top_nouns_weeks.append({\n",
    "            'date': titles_articles['date'],\n",
    "            'top_words': get_top_words(nouns, top_size),\n",
    "        })\n",
    "    return top_nouns_weeks\n",
    "\n",
    "\n",
    "def get_top_words(words, top_size=10):\n",
    "    ''' Сформировать ТОП слов '''\n",
    "    return collections.Counter(words).most_common(top_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0b5e383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "получили 10 страниц с habr.com\n",
      "количество статей: 200\n",
      "+---------------+--------------------------------------------------------------+\n",
      "| Начало недели |                       Популярные слова                       |\n",
      "+===============+==============================================================+\n",
      "|  2023-01-09   |                                                              |\n",
      "+---------------+--------------------------------------------------------------+\n",
      "|  2023-01-10   | pytorch (1), цепочка (1), зависимость (1), дельта (1),       |\n",
      "|               | компрессия (1), квантизация (1), объект (1), c (1)           |\n",
      "+---------------+--------------------------------------------------------------+\n",
      "|               | python (6), сеть (3), функция (2), собеседование (2), бот    |\n",
      "|               | (2), контейнер (2), будущее (2), анализ (2), работа (2),     |\n",
      "|  2023-01-11   | падение (1), продажа (1), пк (1), ноутбук (1), прогноз (1),  |\n",
      "|               | аналитик (1), chatgpt (1), задание (1), волосок (1), кабель  |\n",
      "|               | (1), мир (1)                                                 |\n",
      "+---------------+--------------------------------------------------------------+\n",
      "|               | часть (5), работа (3), процесс (2), страница (2), postgresql |\n",
      "|               | (2), сервис (2), kubernetes (2), год (2), анимация (2),      |\n",
      "|  2023-01-12   | выполнение (2), данные (2), обзор (2), код (2), анализ (2),  |\n",
      "|               | бег (2), перспектива (1), производитель (1), чип (1),        |\n",
      "|               | компания (1), развитие (1)                                   |\n",
      "+---------------+--------------------------------------------------------------+\n",
      "|               | управление (5), часть (4), проблема (2), контроль (2), ия    |\n",
      "|               | (2), openssl (2), c (2), программирование (2), человек (2),  |\n",
      "|  2023-01-13   | obsidian (2), обработка (2), информация (2), работа (2),     |\n",
      "|               | заметка (2), qa (2), linux (2), решение (2), помощь (2),     |\n",
      "|               | компьютер (2), тест (2)                                      |\n",
      "+---------------+--------------------------------------------------------------+\n",
      "|               | часть (2), история (2), реактор (1), источник (1), энергия   |\n",
      "|               | (1), нейросеть (1), электроника (1), восстановление (1),     |\n",
      "|  2023-01-14   | сортировка (1), слияние (1), рекурсия (1), раз (1), crack    |\n",
      "|               | (1), keygen (1), управление (1), микроконтроллер (1),        |\n",
      "|               | telegram-бот (1), связь (1), цикл (1), задача (1)            |\n",
      "+---------------+--------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from texttable import Texttable\n",
    "import argparse\n",
    "\n",
    "def output_words_stat(nouns_weeks):\n",
    "    # Инициализируем таблицу на печать\n",
    "    table = Texttable()\n",
    "    table.set_cols_align(['c', 'l'])\n",
    "    table.set_cols_valign(['m', 'm'])\n",
    "    table.header(['Начало недели', 'Популярные слова'])\n",
    "    for nouns_week in nouns_weeks:\n",
    "        top_words = nouns_week['top_words']\n",
    "        table.add_row((\n",
    "            nouns_week['date'],\n",
    "            ', '.join(['%s (%d)' % (noun, count) for noun, count in top_words])\n",
    "        ))\n",
    "    # Прорисовываем таблицу\n",
    "    print(table.draw())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pages_count = 10\n",
    "top_size = 20\n",
    "\n",
    "habr_pages = fetch_raw_habr_pages(pages_count)\n",
    "print('получили {0} страниц с habr.com'.format(len(habr_pages)))\n",
    "\n",
    "titles_articles = get_titles_articles_with_raw_habr_pages(habr_pages)\n",
    "print('количество статей: {0}'.format(len(titles_articles)))\n",
    "\n",
    "titles_articles_weeks = divide_titles_at_weeks(titles_articles)\n",
    "\n",
    "top_nouns_weeks = parse_nouns_in_titles_articles_at_weeks(\n",
    "    titles_articles_weeks, top_size\n",
    ")\n",
    "\n",
    "output_words_stat(top_nouns_weeks)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
